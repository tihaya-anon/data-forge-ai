# yaml-language-server: $schema=./_schema.json
# 任务示例：实现 Kafka Producer 模块

任务:
  编号: "task-001"
  名称: "实现 Kafka Producer 模块"
  状态: "待处理"
  优先级: "高"
  
  分配:
    agent: "agent1"
    分支: "agent/kafka-producer"
    工作区: "../data-forge-ai-agent1"
    分配时间: null
  
  描述: |
    实现数据管道中的 Kafka 消息生产者模块。
    该模块负责将清洗后的训练数据发送到 Kafka 消息队列，
    供下游的 Flink 流处理作业消费。
  
  目标:
    - "创建 KafkaProducerService 基础类"
    - "实现 JSON 和 Avro 序列化器"
    - "实现批量发送和缓冲机制"
    - "添加重试逻辑（指数退避）"
    - "集成 Prometheus 指标"
    - "编写单元测试和集成测试"
  
  范围:
    允许路径:
      - "src/data-pipeline/kafka/producer/"
      - "tests/data-pipeline/kafka/producer/"
      - "docs/data-pipeline/kafka-producer.md"
    禁止路径:
      - "src/data-pipeline/kafka/consumer/"
      - "src/shared/"
      - "*.lock"
      - ".ai-context/"
  
  依赖:
    前置任务: []
    阻塞任务:
      - "task-003"
  
  接口:
    提供:
      - 名称: "KafkaProducerService"
        类型: "类"
        路径: "src/data-pipeline/kafka/producer/service.py"
    消费:
      - 名称: "MessageSchema"
        类型: "类"
        来源任务: "task-000"
  
  技术要求:
    语言: "python"
    框架: "confluent-kafka"
    设计模式:
      - "工厂模式"
      - "熔断器模式"
    测试:
      必需: true
      最低覆盖率: 80
      类型:
        - "单元测试"
        - "集成测试"
  
  验收标准:
    - "单元测试覆盖率 >= 80%"
    - "集成测试通过"
    - "支持 10000 msg/s 吞吐量"
    - "重试机制正确处理故障"
  
  参考资料:
    文档:
      - "docs/ARCHITECTURE.md#data-pipeline"
    示例:
      - "src/data-pipeline/spark/"
    相关任务:
      - "task-002"
      - "task-003"
  
  进度:
    开始时间: null
    完成时间: null
    预估工时: 8
    实际工时: null
    笔记: []
    阻塞项: []
  
  元信息:
    创建时间: "2024-01-20T10:00:00Z"
    更新时间: "2024-01-20T10:00:00Z"
    创建者: "human"
    版本: 1
